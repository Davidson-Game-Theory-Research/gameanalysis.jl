RD_loses:

Replicator Dynamics found 3 equilibria:
3×3 Matrix{Float64}:
 0.31  1.0  0.32
 0.48  0.0  0.68
 0.21  0.0  0.0
Regrets: [2.95e-5, 0.0, 4.12e-5]


GD_wins:

Gradient Descent found 10 equilibria:
3×10 Matrix{Float64}:
 0.72  0.32  0.2   0.13  0.0   1.0  0.06  0.6   0.31  0.36
 0.23  0.68  0.58  0.71  0.39  0.0  0.24  0.15  0.48  0.6
 0.05  0.0   0.22  0.15  0.61  0.0  0.7   0.25  0.21  0.04
Regrets: [4.68e-5, 4.12e-5, 5.22e-5, -1.14e-13, -1.14e-13, 4.55e-13, 3.62e-6, 1.66e-5, 0.0, 0.0]







RD_wins:

Replicator Dynamics found 4 equilibria:
3×4 Matrix{Float64}:
 0.64  0.83  0.0  0.19
 0.0   0.0   0.0  0.0
 0.36  0.17  1.0  0.81
Regrets: [3.92e-5, 0.0, 0.0, -1.14e-13]


GD_loses:

Gradient Descent found 2 equilibria:
3×2 Matrix{Float64}:
 0.16  0.0
 0.0   0.0
 0.84  1.0
Regrets: [5.13e-5, -1.14e-13]




few_eq:

Replicator Dynamics found 2 equilibria:
3×2 Matrix{Float64}:
 0.0  0.0
 0.0  0.13
 1.0  0.87
Regrets: [0.0, 0.0]


Gradient Descent found 2 equilibria:
3×2 Matrix{Float64}:
 0.04  0.0
 0.67  0.0
 0.3   1.0
Regrets: [2.25e-6, -1.14e-13]


Fictitious Play found 2 equilibria:
3×2 Matrix{Float64}:
 0.0   0.0
 0.13  0.0
 0.87  1.0
Regrets: [7.23e-5, 0.000127]


Iterated Better Response found 2 equilibria:
3×2 Matrix{Float64}:
 0.0   0.0
 0.13  0.0
 0.87  1.0
Regrets: [0.00432, 0.005]
